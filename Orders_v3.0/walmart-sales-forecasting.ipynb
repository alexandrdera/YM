{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Background","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Walmart is a renowned retail corporation that operates a chain of hypermarkets. Here, Walmart has provided a data combining of 45 stores including store information and monthly sales. The data is provided on weekly basis. Walmart tries to find the impact of holidays on the sales of store. For which it has included four holidays’ weeks into the dataset which are Christmas, Thanksgiving, Super bowl, Labor Day. Here we are owing to Analyze the dataset given. Before doing that, let me point out the objective of this analysis. ","metadata":{}},{"cell_type":"markdown","source":"# Business Objectives","metadata":{}},{"cell_type":"markdown","source":"Our Main Objective is to predict sales of store in a week. As in dataset size and time related data are given as feature, so analyze if sales are impacted by time-based factors and space- based factor. Most importantly how inclusion of holidays in a week soars the sales in store? ","metadata":{}},{"cell_type":"markdown","source":"# Importing Necessary Libraries and Data","metadata":{}},{"cell_type":"code","source":"import numpy as np      # To use np.arrays\nimport pandas as pd     # To use dataframes\nfrom pandas.plotting import autocorrelation_plot as auto_corr\n\n# To plot\nimport matplotlib.pyplot as plt  \n%matplotlib inline    \nimport matplotlib as mpl\nimport seaborn as sns\n\n#For date-time\nimport math\nfrom datetime import datetime\nfrom datetime import timedelta\n\n# Another imports if needs\nimport itertools\nimport statsmodels.api as sm\nimport statsmodels.tsa.api as smt\nimport statsmodels.formula.api as smf\n\nfrom sklearn.model_selection import train_test_split\nfrom statsmodels.tsa.seasonal import seasonal_decompose as season\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.metrics import accuracy_score, balanced_accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import metrics\nfrom sklearn.linear_model import LinearRegression \nfrom sklearn import preprocessing\n\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\nfrom statsmodels.tsa.stattools import adfuller, acf, pacf\nfrom statsmodels.tsa.arima_model import ARIMA\n!pip install pmdarima\nfrom pmdarima.utils import decomposed_plot\nfrom pmdarima.arima import decompose\nfrom pmdarima import auto_arima\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:35:55.320993Z","iopub.execute_input":"2022-04-22T05:35:55.321363Z","iopub.status.idle":"2022-04-22T05:36:10.591102Z","shell.execute_reply.started":"2022-04-22T05:35:55.321275Z","shell.execute_reply":"2022-04-22T05:36:10.590237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.options.display.max_columns=100 # to see columns ","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:36:10.593435Z","iopub.execute_input":"2022-04-22T05:36:10.593768Z","iopub.status.idle":"2022-04-22T05:36:10.60123Z","shell.execute_reply.started":"2022-04-22T05:36:10.593699Z","shell.execute_reply":"2022-04-22T05:36:10.600459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_store = pd.read_csv('../input/walmart-sales-forecast/stores.csv') #store data","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:36:10.603165Z","iopub.execute_input":"2022-04-22T05:36:10.603651Z","iopub.status.idle":"2022-04-22T05:36:10.623725Z","shell.execute_reply.started":"2022-04-22T05:36:10.603607Z","shell.execute_reply":"2022-04-22T05:36:10.62304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('../input/walmart-sales-forecast/train.csv') # train set","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:36:10.625116Z","iopub.execute_input":"2022-04-22T05:36:10.625371Z","iopub.status.idle":"2022-04-22T05:36:11.028432Z","shell.execute_reply.started":"2022-04-22T05:36:10.625337Z","shell.execute_reply":"2022-04-22T05:36:11.027213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_features = pd.read_csv('../input/walmart-sales-forecast/features.csv') #external information","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:36:11.035191Z","iopub.execute_input":"2022-04-22T05:36:11.035431Z","iopub.status.idle":"2022-04-22T05:36:11.06575Z","shell.execute_reply.started":"2022-04-22T05:36:11.035401Z","shell.execute_reply":"2022-04-22T05:36:11.065039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# First Look to Data and Merging Three Dataframes","metadata":{}},{"cell_type":"code","source":"df_store.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:36:11.067257Z","iopub.execute_input":"2022-04-22T05:36:11.067534Z","iopub.status.idle":"2022-04-22T05:36:11.086389Z","shell.execute_reply.started":"2022-04-22T05:36:11.067496Z","shell.execute_reply":"2022-04-22T05:36:11.085647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:36:11.087551Z","iopub.execute_input":"2022-04-22T05:36:11.087866Z","iopub.status.idle":"2022-04-22T05:36:11.099914Z","shell.execute_reply.started":"2022-04-22T05:36:11.087819Z","shell.execute_reply":"2022-04-22T05:36:11.099037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_features.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:36:11.101457Z","iopub.execute_input":"2022-04-22T05:36:11.101982Z","iopub.status.idle":"2022-04-22T05:36:11.120651Z","shell.execute_reply.started":"2022-04-22T05:36:11.101941Z","shell.execute_reply":"2022-04-22T05:36:11.119887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# merging 3 different sets\ndf = df_train.merge(df_features, on=['Store', 'Date'], how='inner').merge(df_store, on=['Store'], how='inner')\ndf.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:36:11.122285Z","iopub.execute_input":"2022-04-22T05:36:11.122629Z","iopub.status.idle":"2022-04-22T05:36:11.332406Z","shell.execute_reply.started":"2022-04-22T05:36:11.122589Z","shell.execute_reply":"2022-04-22T05:36:11.331644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(['IsHoliday_y'], axis=1,inplace=True) # removing dublicated column","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:36:11.333862Z","iopub.execute_input":"2022-04-22T05:36:11.334295Z","iopub.status.idle":"2022-04-22T05:36:11.402002Z","shell.execute_reply.started":"2022-04-22T05:36:11.334254Z","shell.execute_reply":"2022-04-22T05:36:11.401167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.rename(columns={'IsHoliday_x':'IsHoliday'},inplace=True) # rename the column","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:36:11.40342Z","iopub.execute_input":"2022-04-22T05:36:11.403785Z","iopub.status.idle":"2022-04-22T05:36:11.409174Z","shell.execute_reply.started":"2022-04-22T05:36:11.403745Z","shell.execute_reply":"2022-04-22T05:36:11.408374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head() # last ready data set","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-22T05:36:11.410847Z","iopub.execute_input":"2022-04-22T05:36:11.411122Z","iopub.status.idle":"2022-04-22T05:36:11.436105Z","shell.execute_reply.started":"2022-04-22T05:36:11.411085Z","shell.execute_reply":"2022-04-22T05:36:11.435376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:36:11.437578Z","iopub.execute_input":"2022-04-22T05:36:11.437964Z","iopub.status.idle":"2022-04-22T05:36:11.446768Z","shell.execute_reply.started":"2022-04-22T05:36:11.437923Z","shell.execute_reply":"2022-04-22T05:36:11.445799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Store & Department Numbers","metadata":{}},{"cell_type":"code","source":"df['Store'].nunique() # number of different values","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:36:11.452107Z","iopub.execute_input":"2022-04-22T05:36:11.452321Z","iopub.status.idle":"2022-04-22T05:36:11.462457Z","shell.execute_reply.started":"2022-04-22T05:36:11.452289Z","shell.execute_reply":"2022-04-22T05:36:11.461605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Dept'].nunique() # number of different values","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-22T05:36:11.464091Z","iopub.execute_input":"2022-04-22T05:36:11.464598Z","iopub.status.idle":"2022-04-22T05:36:11.474779Z","shell.execute_reply.started":"2022-04-22T05:36:11.46456Z","shell.execute_reply":"2022-04-22T05:36:11.473795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, I will look at the average weekly sales for each store and each department to see if there is any weird values or not. There are 45 stores and 81 departments for stores. ","metadata":{}},{"cell_type":"code","source":"store_dept_table = pd.pivot_table(df, index='Store', columns='Dept',\n                                  values='Weekly_Sales', aggfunc=np.mean)\ndisplay(store_dept_table)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:36:11.476222Z","iopub.execute_input":"2022-04-22T05:36:11.476515Z","iopub.status.idle":"2022-04-22T05:36:11.705763Z","shell.execute_reply.started":"2022-04-22T05:36:11.476477Z","shell.execute_reply":"2022-04-22T05:36:11.705034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Store numbers begin from 1 to 45, department numbers are from 1 to 99, but some numbers are missing such as there is no 88 or 89 etc. Total number of departments is 81. \n\nFrom the pivot table, it is obviously seen that there are some wrong values such as there are 0 and minus values for weekly sales. But sales amount can not be minus. Also, it is impossible for one department not to sell anything whole week. So, I will change this values.","metadata":{}},{"cell_type":"code","source":"df.loc[df['Weekly_Sales']<=0]","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:36:11.706852Z","iopub.execute_input":"2022-04-22T05:36:11.707104Z","iopub.status.idle":"2022-04-22T05:36:11.741554Z","shell.execute_reply.started":"2022-04-22T05:36:11.707069Z","shell.execute_reply":"2022-04-22T05:36:11.740707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1358 rows in 421570 rows means 0.3%, so I can delete and ignore these rows which contains wrong sales values.","metadata":{}},{"cell_type":"code","source":"df = df.loc[df['Weekly_Sales'] > 0]","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:36:11.743104Z","iopub.execute_input":"2022-04-22T05:36:11.743432Z","iopub.status.idle":"2022-04-22T05:36:11.777507Z","shell.execute_reply.started":"2022-04-22T05:36:11.743394Z","shell.execute_reply":"2022-04-22T05:36:11.776679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape # new data shape","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:36:11.778865Z","iopub.execute_input":"2022-04-22T05:36:11.77921Z","iopub.status.idle":"2022-04-22T05:36:11.7868Z","shell.execute_reply.started":"2022-04-22T05:36:11.779172Z","shell.execute_reply":"2022-04-22T05:36:11.785875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Date","metadata":{}},{"cell_type":"code","source":"df['Date'].head(5).append(df['Date'].tail(5)) # to see first and last 5 rows.","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:36:11.788482Z","iopub.execute_input":"2022-04-22T05:36:11.789057Z","iopub.status.idle":"2022-04-22T05:36:11.799342Z","shell.execute_reply.started":"2022-04-22T05:36:11.789017Z","shell.execute_reply":"2022-04-22T05:36:11.798508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our data is from 5th of February 2010 to 26th of October 2012.  ","metadata":{}},{"cell_type":"markdown","source":"# IsHoliday column","metadata":{}},{"cell_type":"code","source":"sns.barplot(x='IsHoliday', y='Weekly_Sales', data=df)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-22T05:36:11.80121Z","iopub.execute_input":"2022-04-22T05:36:11.801876Z","iopub.status.idle":"2022-04-22T05:36:17.023362Z","shell.execute_reply.started":"2022-04-22T05:36:11.801835Z","shell.execute_reply":"2022-04-22T05:36:17.022589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_holiday = df.loc[df['IsHoliday']==True]\ndf_holiday['Date'].unique() ","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:36:17.02496Z","iopub.execute_input":"2022-04-22T05:36:17.025484Z","iopub.status.idle":"2022-04-22T05:36:17.04134Z","shell.execute_reply.started":"2022-04-22T05:36:17.02544Z","shell.execute_reply":"2022-04-22T05:36:17.040195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_not_holiday = df.loc[df['IsHoliday']==False]\ndf_not_holiday['Date'].nunique() ","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:36:17.042794Z","iopub.execute_input":"2022-04-22T05:36:17.043239Z","iopub.status.idle":"2022-04-22T05:36:17.110521Z","shell.execute_reply.started":"2022-04-22T05:36:17.043199Z","shell.execute_reply":"2022-04-22T05:36:17.109762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All holidays are not in the data. There are 4 holiday values such as;\n\nSuper Bowl: 12-Feb-10, 11-Feb-11, 10-Feb-12, 8-Feb-13\n\nLabor Day: 10-Sep-10, 9-Sep-11, 7-Sep-12, 6-Sep-13\n\nThanksgiving: 26-Nov-10, 25-Nov-11, 23-Nov-12, 29-Nov-13\n\nChristmas: 31-Dec-10, 30-Dec-11, 28-Dec-12, 27-Dec-13\n\n\nAfter the 07-Sep-2012 holidays are in test set for prediction. When we look at the data, average weekly sales for holidays are significantly higher than not-holiday days. In train data, there are 133 weeks for non-holiday and 10 weeks for holiday.","metadata":{}},{"cell_type":"markdown","source":"I want to see differences between holiday types. So, I create new columns for 4 types of holidays and fill them with boolean values. If date belongs to this type of holiday it is True, if not False. ","metadata":{}},{"cell_type":"code","source":"# Super bowl dates in train set\ndf.loc[(df['Date'] == '2010-02-12')|(df['Date'] == '2011-02-11')|(df['Date'] == '2012-02-10'),'Super_Bowl'] = True\ndf.loc[(df['Date'] != '2010-02-12')&(df['Date'] != '2011-02-11')&(df['Date'] != '2012-02-10'),'Super_Bowl'] = False","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:36:17.112027Z","iopub.execute_input":"2022-04-22T05:36:17.112456Z","iopub.status.idle":"2022-04-22T05:36:17.567184Z","shell.execute_reply.started":"2022-04-22T05:36:17.112415Z","shell.execute_reply":"2022-04-22T05:36:17.566287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Labor day dates in train set\ndf.loc[(df['Date'] == '2010-09-10')|(df['Date'] == '2011-09-09')|(df['Date'] == '2012-09-07'),'Labor_Day'] = True\ndf.loc[(df['Date'] != '2010-09-10')&(df['Date'] != '2011-09-09')&(df['Date'] != '2012-09-07'),'Labor_Day'] = False","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:36:17.568887Z","iopub.execute_input":"2022-04-22T05:36:17.569351Z","iopub.status.idle":"2022-04-22T05:36:18.056123Z","shell.execute_reply.started":"2022-04-22T05:36:17.569305Z","shell.execute_reply":"2022-04-22T05:36:18.055164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Thanksgiving dates in train set\ndf.loc[(df['Date'] == '2010-11-26')|(df['Date'] == '2011-11-25'),'Thanksgiving'] = True\ndf.loc[(df['Date'] != '2010-11-26')&(df['Date'] != '2011-11-25'),'Thanksgiving'] = False","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:36:18.057728Z","iopub.execute_input":"2022-04-22T05:36:18.058232Z","iopub.status.idle":"2022-04-22T05:36:18.503178Z","shell.execute_reply.started":"2022-04-22T05:36:18.058191Z","shell.execute_reply":"2022-04-22T05:36:18.502324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Christmas dates in train set\ndf.loc[(df['Date'] == '2010-12-31')|(df['Date'] == '2011-12-30'),'Christmas'] = True\ndf.loc[(df['Date'] != '2010-12-31')&(df['Date'] != '2011-12-30'),'Christmas'] = False","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:36:18.508037Z","iopub.execute_input":"2022-04-22T05:36:18.510912Z","iopub.status.idle":"2022-04-22T05:36:18.85323Z","shell.execute_reply.started":"2022-04-22T05:36:18.510862Z","shell.execute_reply":"2022-04-22T05:36:18.852426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.barplot(x='Christmas', y='Weekly_Sales', data=df) # Christmas holiday vs not-Christmas","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-22T05:36:18.854527Z","iopub.execute_input":"2022-04-22T05:36:18.854846Z","iopub.status.idle":"2022-04-22T05:36:24.379839Z","shell.execute_reply.started":"2022-04-22T05:36:18.854806Z","shell.execute_reply":"2022-04-22T05:36:24.379088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.barplot(x='Thanksgiving', y='Weekly_Sales', data=df) # Thanksgiving holiday vs not-thanksgiving","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:36:24.381313Z","iopub.execute_input":"2022-04-22T05:36:24.381585Z","iopub.status.idle":"2022-04-22T05:36:30.554564Z","shell.execute_reply.started":"2022-04-22T05:36:24.381545Z","shell.execute_reply":"2022-04-22T05:36:30.553669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.barplot(x='Super_Bowl', y='Weekly_Sales', data=df) # Super bowl holiday vs not-super bowl","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-22T05:36:30.55607Z","iopub.execute_input":"2022-04-22T05:36:30.55704Z","iopub.status.idle":"2022-04-22T05:36:36.056323Z","shell.execute_reply.started":"2022-04-22T05:36:30.556991Z","shell.execute_reply":"2022-04-22T05:36:36.055596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.barplot(x='Labor_Day', y='Weekly_Sales', data=df) # Labor day holiday vs not-labor day","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:36:36.05764Z","iopub.execute_input":"2022-04-22T05:36:36.058102Z","iopub.status.idle":"2022-04-22T05:36:41.91872Z","shell.execute_reply.started":"2022-04-22T05:36:36.058058Z","shell.execute_reply":"2022-04-22T05:36:41.917765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is shown that for the graphs, Labor Day and Christmas do not increase weekly average sales. There is positive effect on sales in Super bowl, but the highest difference is in the Thanksgiving. I think, people generally prefer to buy Christmas gifts 1-2 weeks before Christmas, so it does not change sales in the Christmas week. And, there is Black Friday sales in the Thanksgiving week.","metadata":{}},{"cell_type":"markdown","source":"# Type Effect on Holidays","metadata":{}},{"cell_type":"markdown","source":"There are three different store types in the data as A, B and C.","metadata":{}},{"cell_type":"code","source":"df.groupby(['Christmas','Type'])['Weekly_Sales'].mean()  # Avg weekly sales for types on Christmas ","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:36:41.920384Z","iopub.execute_input":"2022-04-22T05:36:41.920878Z","iopub.status.idle":"2022-04-22T05:36:42.019675Z","shell.execute_reply.started":"2022-04-22T05:36:41.920834Z","shell.execute_reply":"2022-04-22T05:36:42.018768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.groupby(['Labor_Day','Type'])['Weekly_Sales'].mean()  # Avg weekly sales for types on Labor Day","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:36:42.023268Z","iopub.execute_input":"2022-04-22T05:36:42.025058Z","iopub.status.idle":"2022-04-22T05:36:42.114944Z","shell.execute_reply.started":"2022-04-22T05:36:42.024953Z","shell.execute_reply":"2022-04-22T05:36:42.113811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.groupby(['Thanksgiving','Type'])['Weekly_Sales'].mean()  # Avg weekly sales for types on Thanksgiving","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:36:42.116652Z","iopub.execute_input":"2022-04-22T05:36:42.117051Z","iopub.status.idle":"2022-04-22T05:36:42.205231Z","shell.execute_reply.started":"2022-04-22T05:36:42.117005Z","shell.execute_reply":"2022-04-22T05:36:42.204305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.groupby(['Super_Bowl','Type'])['Weekly_Sales'].mean()  # Avg weekly sales for types on Super Bowl","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:36:42.2071Z","iopub.execute_input":"2022-04-22T05:36:42.207391Z","iopub.status.idle":"2022-04-22T05:36:42.293743Z","shell.execute_reply.started":"2022-04-22T05:36:42.207352Z","shell.execute_reply":"2022-04-22T05:36:42.292864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I want to see percentages of store types.","metadata":{}},{"cell_type":"code","source":"my_data = [48.88, 37.77 , 13.33 ]  #percentages\nmy_labels = 'Type A','Type B', 'Type C' # labels\nplt.pie(my_data,labels=my_labels,autopct='%1.1f%%', textprops={'fontsize': 15}) #plot pie type and bigger the labels\nplt.axis('equal')\nmpl.rcParams.update({'font.size': 20}) #bigger percentage labels\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:36:42.29564Z","iopub.execute_input":"2022-04-22T05:36:42.296074Z","iopub.status.idle":"2022-04-22T05:36:42.420353Z","shell.execute_reply.started":"2022-04-22T05:36:42.296028Z","shell.execute_reply":"2022-04-22T05:36:42.419133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.groupby('IsHoliday')['Weekly_Sales'].mean()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:36:42.422428Z","iopub.execute_input":"2022-04-22T05:36:42.423052Z","iopub.status.idle":"2022-04-22T05:36:42.454352Z","shell.execute_reply.started":"2022-04-22T05:36:42.423005Z","shell.execute_reply":"2022-04-22T05:36:42.453421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nearly, half of the stores are belongs to Type A.","metadata":{}},{"cell_type":"code","source":"# Plotting avg wekkly sales according to holidays by types\nplt.style.use('seaborn-poster')\nlabels = ['Thanksgiving', 'Super_Bowl', 'Labor_Day', 'Christmas']\nA_means = [27397.77, 20612.75, 20004.26, 18310.16]\nB_means = [18733.97, 12463.41, 12080.75, 11483.97]\nC_means = [9696.56,10179.27,9893.45,8031.52]\n\nx = np.arange(len(labels))  # the label locations\nwidth = 0.25  # the width of the bars\n\nfig, ax = plt.subplots(figsize=(16, 8))\nrects1 = ax.bar(x - width, A_means, width, label='Type_A')\nrects2 = ax.bar(x , B_means, width, label='Type_B')\nrects3 = ax.bar(x + width, C_means, width, label='Type_C')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_ylabel('Weekly Avg Sales')\nax.set_xticks(x)\nax.set_xticklabels(labels)\nax.legend()\n\n\ndef autolabel(rects):\n    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n    for rect in rects:\n        height = rect.get_height()\n        ax.annotate('{}'.format(height),\n                    xy=(rect.get_x() + rect.get_width() / 2, height),\n                    xytext=(0, 3),  # 3 points vertical offset\n                    textcoords=\"offset points\",\n                    ha='center', va='bottom')\n\n\nautolabel(rects1)\nautolabel(rects2)\nautolabel(rects3)\n\nplt.axhline(y=17094.30,color='r') # holidays avg\nplt.axhline(y=15952.82,color='green') # not-holiday avg\n\nfig.tight_layout()\n\nplt.show()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-22T05:36:42.458205Z","iopub.execute_input":"2022-04-22T05:36:42.46234Z","iopub.status.idle":"2022-04-22T05:36:42.915454Z","shell.execute_reply.started":"2022-04-22T05:36:42.46229Z","shell.execute_reply":"2022-04-22T05:36:42.91471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is seen from the graph that, highest sale average is in the Thanksgiving week between holidays. And, for all holidays Type A stores has highest sales.","metadata":{}},{"cell_type":"code","source":"df.sort_values(by='Weekly_Sales',ascending=False).head(5)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-22T05:36:42.916832Z","iopub.execute_input":"2022-04-22T05:36:42.917213Z","iopub.status.idle":"2022-04-22T05:36:43.090303Z","shell.execute_reply.started":"2022-04-22T05:36:42.917171Z","shell.execute_reply":"2022-04-22T05:36:43.089518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Also, it is not surprise that top 5 highest weekly sales are belongs to Thanksgiving weeks.","metadata":{}},{"cell_type":"markdown","source":"# To See the Size - Type Relation","metadata":{}},{"cell_type":"code","source":"df_store.groupby('Type').describe()['Size'].round(2) # See the Size-Type relation","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:36:43.091605Z","iopub.execute_input":"2022-04-22T05:36:43.092504Z","iopub.status.idle":"2022-04-22T05:36:43.135556Z","shell.execute_reply.started":"2022-04-22T05:36:43.092461Z","shell.execute_reply":"2022-04-22T05:36:43.134725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,8)) # To see the type-size relation\nfig = sns.boxplot(x='Type', y='Size', data=df, showfliers=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:36:43.144467Z","iopub.execute_input":"2022-04-22T05:36:43.144696Z","iopub.status.idle":"2022-04-22T05:36:43.67721Z","shell.execute_reply.started":"2022-04-22T05:36:43.144667Z","shell.execute_reply":"2022-04-22T05:36:43.676461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Size of the type of stores are consistent with sales, as expected. Higher size stores has higher sales. And, Walmart classify stores according to their sizes according to graph. After the smallest size value of Type A, Type B begins. After the smallest size value of Type B, Type C begins.","metadata":{}},{"cell_type":"markdown","source":"# Markdown Columns","metadata":{}},{"cell_type":"markdown","source":"Walmart gave markdown columns to see the effect if markdowns on sales. When I check columns, there are many NaN values for markdowns. I decided to change them with 0, because if there is markdown in the row, it is shown with numbres. So, if I can write 0, it shows there is no markdown at that date.","metadata":{}},{"cell_type":"code","source":"df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:36:43.678494Z","iopub.execute_input":"2022-04-22T05:36:43.679094Z","iopub.status.idle":"2022-04-22T05:36:43.976721Z","shell.execute_reply.started":"2022-04-22T05:36:43.679052Z","shell.execute_reply":"2022-04-22T05:36:43.97594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.fillna(0) # filling null's with 0","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:36:43.978208Z","iopub.execute_input":"2022-04-22T05:36:43.978663Z","iopub.status.idle":"2022-04-22T05:36:44.412598Z","shell.execute_reply.started":"2022-04-22T05:36:43.978618Z","shell.execute_reply":"2022-04-22T05:36:44.41179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isna().sum() # last null check","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:36:44.414147Z","iopub.execute_input":"2022-04-22T05:36:44.414707Z","iopub.status.idle":"2022-04-22T05:36:44.527294Z","shell.execute_reply.started":"2022-04-22T05:36:44.414662Z","shell.execute_reply":"2022-04-22T05:36:44.526389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe() # to see weird statistical things","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:36:44.528905Z","iopub.execute_input":"2022-04-22T05:36:44.529198Z","iopub.status.idle":"2022-04-22T05:36:44.703887Z","shell.execute_reply.started":"2022-04-22T05:36:44.529156Z","shell.execute_reply":"2022-04-22T05:36:44.702977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Minimum value for weekly sales is 0.01. Most probably, this value is not true but I prefer not to change them now. Because, there are many departments and many stores. It takes too much time to check each department for each store (45 store for 81 departments). So, I take averages for EDA. ","metadata":{}},{"cell_type":"markdown","source":"# Deeper Look in Sales","metadata":{}},{"cell_type":"code","source":"x = df['Dept']\ny = df['Weekly_Sales']\nplt.figure(figsize=(15,5))\nplt.title('Weekly Sales by Department')\nplt.xlabel('Departments')\nplt.ylabel('Weekly Sales')\nplt.scatter(x,y)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:36:44.7055Z","iopub.execute_input":"2022-04-22T05:36:44.7058Z","iopub.status.idle":"2022-04-22T05:36:46.27343Z","shell.execute_reply.started":"2022-04-22T05:36:44.705761Z","shell.execute_reply":"2022-04-22T05:36:46.272639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(30,10))\nfig = sns.barplot(x='Dept', y='Weekly_Sales', data=df)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-22T05:36:46.274932Z","iopub.execute_input":"2022-04-22T05:36:46.275215Z","iopub.status.idle":"2022-04-22T05:36:54.930423Z","shell.execute_reply.started":"2022-04-22T05:36:46.275176Z","shell.execute_reply":"2022-04-22T05:36:54.929634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the first graph, it is seen that one department between 60-80(I assume it is 72), has higher sales values. But, when we take the averages, it is seen that department 92 has higher mean sales. Department 72 is seasonal department, I think. It has higher values is some seasons but on average 92 is higher.","metadata":{}},{"cell_type":"code","source":"x = df['Store']\ny = df['Weekly_Sales']\nplt.figure(figsize=(15,5))\nplt.title('Weekly Sales by Store')\nplt.xlabel('Stores')\nplt.ylabel('Weekly Sales')\nplt.scatter(x,y)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:36:54.931868Z","iopub.execute_input":"2022-04-22T05:36:54.933435Z","iopub.status.idle":"2022-04-22T05:36:56.48095Z","shell.execute_reply.started":"2022-04-22T05:36:54.933389Z","shell.execute_reply":"2022-04-22T05:36:56.480189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,6))\nfig = sns.barplot(x='Store', y='Weekly_Sales', data=df)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:36:56.482128Z","iopub.execute_input":"2022-04-22T05:36:56.482599Z","iopub.status.idle":"2022-04-22T05:37:03.835427Z","shell.execute_reply.started":"2022-04-22T05:36:56.482558Z","shell.execute_reply":"2022-04-22T05:37:03.834622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Same thing happens in stores. From the first graph, some stores has higher sales but on average store 20 is the best and 4 and 14 following it.","metadata":{}},{"cell_type":"markdown","source":"# Changing Date to Datetime and Creating New Columns","metadata":{}},{"cell_type":"code","source":"df[\"Date\"] = pd.to_datetime(df[\"Date\"]) # convert to datetime\ndf['week'] =df['Date'].dt.week\ndf['month'] =df['Date'].dt.month \ndf['year'] =df['Date'].dt.year","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:37:03.837053Z","iopub.execute_input":"2022-04-22T05:37:03.837365Z","iopub.status.idle":"2022-04-22T05:37:04.146496Z","shell.execute_reply.started":"2022-04-22T05:37:03.837323Z","shell.execute_reply":"2022-04-22T05:37:04.145641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.groupby('month')['Weekly_Sales'].mean() # to see the best months for sales","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:37:04.14816Z","iopub.execute_input":"2022-04-22T05:37:04.148481Z","iopub.status.idle":"2022-04-22T05:37:04.167475Z","shell.execute_reply.started":"2022-04-22T05:37:04.148442Z","shell.execute_reply":"2022-04-22T05:37:04.166713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.groupby('year')['Weekly_Sales'].mean() # to see the best years for sales","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:37:04.16868Z","iopub.execute_input":"2022-04-22T05:37:04.168997Z","iopub.status.idle":"2022-04-22T05:37:04.187634Z","shell.execute_reply.started":"2022-04-22T05:37:04.168956Z","shell.execute_reply":"2022-04-22T05:37:04.186912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"monthly_sales = pd.pivot_table(df, values = \"Weekly_Sales\", columns = \"year\", index = \"month\")\nmonthly_sales.plot()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:37:04.188723Z","iopub.execute_input":"2022-04-22T05:37:04.189529Z","iopub.status.idle":"2022-04-22T05:37:04.520332Z","shell.execute_reply.started":"2022-04-22T05:37:04.189488Z","shell.execute_reply":"2022-04-22T05:37:04.519588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the graph, it is seen that 2011 has lower sales than 2010 generally. When we look at the mean sales it is seen that 2010 has higher values, but 2012 has no information about November and December which have higher sales. Despite of 2012 has no last two months sales, it's mean is near to 2010. Most probably, it will take the first place if we get 2012 results and add them.","metadata":{}},{"cell_type":"code","source":"fig = sns.barplot(x='month', y='Weekly_Sales', data=df)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:37:04.521824Z","iopub.execute_input":"2022-04-22T05:37:04.522349Z","iopub.status.idle":"2022-04-22T05:37:09.493489Z","shell.execute_reply.started":"2022-04-22T05:37:04.522306Z","shell.execute_reply":"2022-04-22T05:37:09.492603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When we look at the graph above, the best sales are in December and November, as expected. The highest values are belongs to Thankgiving holiday but when we take average it is obvious that December has the best value.","metadata":{}},{"cell_type":"code","source":"df.groupby('week')['Weekly_Sales'].mean().sort_values(ascending=False).head()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:37:09.494838Z","iopub.execute_input":"2022-04-22T05:37:09.495118Z","iopub.status.idle":"2022-04-22T05:37:09.513485Z","shell.execute_reply.started":"2022-04-22T05:37:09.495077Z","shell.execute_reply":"2022-04-22T05:37:09.512676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Top 5 sales averages by weekly belongs to 1-2 weeks before Christmas, Thanksgiving, Black Friday and end of May, when the schools are closed. ","metadata":{}},{"cell_type":"code","source":"weekly_sales = pd.pivot_table(df, values = \"Weekly_Sales\", columns = \"year\", index = \"week\")\nweekly_sales.plot()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:37:09.514931Z","iopub.execute_input":"2022-04-22T05:37:09.515481Z","iopub.status.idle":"2022-04-22T05:37:09.980529Z","shell.execute_reply.started":"2022-04-22T05:37:09.515436Z","shell.execute_reply":"2022-04-22T05:37:09.97978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,6))\nfig = sns.barplot(x='week', y='Weekly_Sales', data=df)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:37:09.981768Z","iopub.execute_input":"2022-04-22T05:37:09.982379Z","iopub.status.idle":"2022-04-22T05:37:17.333759Z","shell.execute_reply.started":"2022-04-22T05:37:09.982333Z","shell.execute_reply":"2022-04-22T05:37:17.332987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From graphs, it is seen that 51th week and 47th weeks have significantly higher averages as Christmas, Thankgiving and Black Friday effects.","metadata":{}},{"cell_type":"markdown","source":"# Fuel Price, CPI , Unemployment , Temperature Effects","metadata":{}},{"cell_type":"code","source":"fuel_price = pd.pivot_table(df, values = \"Weekly_Sales\", index= \"Fuel_Price\")\nfuel_price.plot()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:37:17.33508Z","iopub.execute_input":"2022-04-22T05:37:17.335702Z","iopub.status.idle":"2022-04-22T05:37:17.64033Z","shell.execute_reply.started":"2022-04-22T05:37:17.335655Z","shell.execute_reply":"2022-04-22T05:37:17.639591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = pd.pivot_table(df, values = \"Weekly_Sales\", index= \"Temperature\")\ntemp.plot()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:37:17.641866Z","iopub.execute_input":"2022-04-22T05:37:17.642319Z","iopub.status.idle":"2022-04-22T05:37:17.975351Z","shell.execute_reply.started":"2022-04-22T05:37:17.642278Z","shell.execute_reply":"2022-04-22T05:37:17.974608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CPI = pd.pivot_table(df, values = \"Weekly_Sales\", index= \"CPI\")\nCPI.plot()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:37:17.976966Z","iopub.execute_input":"2022-04-22T05:37:17.977512Z","iopub.status.idle":"2022-04-22T05:37:18.279367Z","shell.execute_reply.started":"2022-04-22T05:37:17.977469Z","shell.execute_reply":"2022-04-22T05:37:18.278633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unemployment = pd.pivot_table(df, values = \"Weekly_Sales\", index= \"Unemployment\")\nunemployment.plot()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-22T05:37:18.280904Z","iopub.execute_input":"2022-04-22T05:37:18.28141Z","iopub.status.idle":"2022-04-22T05:37:18.567376Z","shell.execute_reply.started":"2022-04-22T05:37:18.281367Z","shell.execute_reply":"2022-04-22T05:37:18.566625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From graphs, it is seen that there are no significant patterns between CPI, temperature, unemployment rate, fuel price vs weekly sales. There is no data for CPI between 140-180 also.","metadata":{}},{"cell_type":"code","source":"df.to_csv('clean_data.csv') # assign new data frame to csv for using after here","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:37:18.568711Z","iopub.execute_input":"2022-04-22T05:37:18.569532Z","iopub.status.idle":"2022-04-22T05:37:27.503905Z","shell.execute_reply.started":"2022-04-22T05:37:18.569487Z","shell.execute_reply":"2022-04-22T05:37:27.503019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Findings and Explorations","metadata":{}},{"cell_type":"markdown","source":"# Cleaning Process","metadata":{}},{"cell_type":"markdown","source":"- The data has no too much missing values. All columns was checked. \n- I choose rows which has higher than 0 weekly sales. Minus values are 0.3% of data. So, I dropped them.\n- Null values in markdowns changed to zero. Because, they were written as null if there were no markdown on this department. ","metadata":{}},{"cell_type":"markdown","source":"# Explorations & Findings","metadata":{}},{"cell_type":"markdown","source":"- There are 45 stores and 81 department in data. Departments are not same in all stores. \n- Although department 72 has higher weekly sales values, on average department 92 is the best. It shows us, some departments has higher values as seasonal like Thanksgiving. It is consistant when we look at the top 5 sales in data, all of them belongs to 72th department at Thanksgiving holiday time. \n- Although stores 10 and 35 have higher weekly sales values sometimes, in general average store 20 and store 4 are on the first and second rank. It means that some areas has higher seasonal sales. \n- Stores has 3 types as A, B and C according to their sizes. Almost half of the stores are bigger than 150000 and categorized as A. According to type, sales of the stores are changing.\n- As expected, holiday average sales are higher than normal dates.\n- Christmas holiday introduces as the last days of the year. But people generally shop at 51th week. So, when we look at the total sales of holidays, Thankgiving has higher sales between them which was assigned by Walmart.\n- Year 2010 has higher sales than 2011 and 2012. But, November and December sales are not in the data for 2012. Even without highest sale months, 2012 is not significantly less than 2010, so after adding last two months, it can be first.\n- It is obviously seen that week 51 and 47 have higher values and 50-48 weeks follow them. Interestingly, 5th top sales belongs to 22th week of the year. This results show that Christmas, Thankgiving and Black Friday are very important than other weeks for sales and 5th important time is 22th week of the year and it is end of the May, when schools are closed. Most probably, people are preparing for holiday at the end of the May. \n- January sales are significantly less than other months. This is the result of November and December high sales. After two high sales month, people prefer to pay less on January.\n- CPI, temperature, unemployment rate and fuel price have no pattern on weekly sales. \n","metadata":{}},{"cell_type":"markdown","source":"# First Trial with Random Forest","metadata":{}},{"cell_type":"markdown","source":"Generally, Rondom Forest Regressor gives good results when we tune it well. So, to find simple baseline model, I will use RandomForestRegressor in this notebook. Also, feature importance for model can be found in this notebook. \n\nOur metric for this project is weighted mean absolute error (WMAE):","metadata":{}},{"cell_type":"markdown","source":"![title](https://miro.medium.com/max/990/1*VKYKK85ViLYUUjyOWVURfw.jpeg)","metadata":{}},{"cell_type":"markdown","source":"where\n\n- n is the number of rows\n- ŷ i is the predicted sales\n- yi is the actual sales\n- wi are weights. w = 5 if the week is a holiday week, 1 otherwise","metadata":{}},{"cell_type":"markdown","source":"With this metric, the error at holiday weeks has 5 times weight more than normal weeks. So, it is more important to predict sales at holiday weeks accurately.\nAll results for trails can be found at the end of this notebook.","metadata":{}},{"cell_type":"code","source":"pd.options.display.max_columns=100 # to see columns ","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:37:27.505389Z","iopub.execute_input":"2022-04-22T05:37:27.505665Z","iopub.status.idle":"2022-04-22T05:37:27.511289Z","shell.execute_reply.started":"2022-04-22T05:37:27.505627Z","shell.execute_reply":"2022-04-22T05:37:27.510391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('./clean_data.csv')","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:37:27.513139Z","iopub.execute_input":"2022-04-22T05:37:27.513886Z","iopub.status.idle":"2022-04-22T05:37:28.645902Z","shell.execute_reply.started":"2022-04-22T05:37:27.513837Z","shell.execute_reply":"2022-04-22T05:37:28.645034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(columns=['Unnamed: 0'],inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:37:28.647506Z","iopub.execute_input":"2022-04-22T05:37:28.647846Z","iopub.status.idle":"2022-04-22T05:37:28.674081Z","shell.execute_reply.started":"2022-04-22T05:37:28.647795Z","shell.execute_reply":"2022-04-22T05:37:28.672914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Date'] = pd.to_datetime(df['Date']) # changing datetime to divide if needs","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:37:28.675852Z","iopub.execute_input":"2022-04-22T05:37:28.676204Z","iopub.status.idle":"2022-04-22T05:37:28.768675Z","shell.execute_reply.started":"2022-04-22T05:37:28.676161Z","shell.execute_reply":"2022-04-22T05:37:28.767852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encoding the Data ","metadata":{}},{"cell_type":"markdown","source":"For preprocessing our data, I will change holidays boolean values to 0-1 and replace type of the stores from A, B, C to 1, 2, 3. ","metadata":{}},{"cell_type":"code","source":"df_encoded = df.copy() # to keep original dataframe taking copy of it","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:37:28.769912Z","iopub.execute_input":"2022-04-22T05:37:28.77059Z","iopub.status.idle":"2022-04-22T05:37:28.796927Z","shell.execute_reply.started":"2022-04-22T05:37:28.770547Z","shell.execute_reply":"2022-04-22T05:37:28.796087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type_group = {'A':1, 'B': 2, 'C': 3}  # changing A,B,C to 1-2-3\ndf_encoded['Type'] = df_encoded['Type'].replace(type_group)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:37:28.798367Z","iopub.execute_input":"2022-04-22T05:37:28.798826Z","iopub.status.idle":"2022-04-22T05:37:29.069546Z","shell.execute_reply.started":"2022-04-22T05:37:28.798776Z","shell.execute_reply":"2022-04-22T05:37:29.068754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_encoded['Super_Bowl'] = df_encoded['Super_Bowl'].astype(bool).astype(int) # changing T,F to 0-1","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:37:29.07282Z","iopub.execute_input":"2022-04-22T05:37:29.073062Z","iopub.status.idle":"2022-04-22T05:37:29.080717Z","shell.execute_reply.started":"2022-04-22T05:37:29.073032Z","shell.execute_reply":"2022-04-22T05:37:29.079731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_encoded['Thanksgiving'] = df_encoded['Thanksgiving'].astype(bool).astype(int) # changing T,F to 0-1","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:37:29.082257Z","iopub.execute_input":"2022-04-22T05:37:29.082902Z","iopub.status.idle":"2022-04-22T05:37:29.093472Z","shell.execute_reply.started":"2022-04-22T05:37:29.082861Z","shell.execute_reply":"2022-04-22T05:37:29.092552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_encoded['Labor_Day'] = df_encoded['Labor_Day'].astype(bool).astype(int) # changing T,F to 0-1","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:37:29.095294Z","iopub.execute_input":"2022-04-22T05:37:29.095617Z","iopub.status.idle":"2022-04-22T05:37:29.105028Z","shell.execute_reply.started":"2022-04-22T05:37:29.095576Z","shell.execute_reply":"2022-04-22T05:37:29.104091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_encoded['Christmas'] = df_encoded['Christmas'].astype(bool).astype(int) # changing T,F to 0-1","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:37:29.106612Z","iopub.execute_input":"2022-04-22T05:37:29.107174Z","iopub.status.idle":"2022-04-22T05:37:29.115877Z","shell.execute_reply.started":"2022-04-22T05:37:29.107073Z","shell.execute_reply":"2022-04-22T05:37:29.115032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_encoded['IsHoliday'] = df_encoded['IsHoliday'].astype(bool).astype(int) # changing T,F to 0-1","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:37:29.119199Z","iopub.execute_input":"2022-04-22T05:37:29.11947Z","iopub.status.idle":"2022-04-22T05:37:29.128669Z","shell.execute_reply.started":"2022-04-22T05:37:29.119439Z","shell.execute_reply":"2022-04-22T05:37:29.127787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_new = df_encoded.copy() # taking the copy of encoded df to keep it original","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:37:29.131888Z","iopub.execute_input":"2022-04-22T05:37:29.132377Z","iopub.status.idle":"2022-04-22T05:37:29.198168Z","shell.execute_reply.started":"2022-04-22T05:37:29.132337Z","shell.execute_reply":"2022-04-22T05:37:29.197277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Observation of Interactions between Features","metadata":{}},{"cell_type":"markdown","source":"Firstly, i will drop divided holiday columns from my data and try without them. To keep my encoded data safe, I assigned my dataframe to new one and I will use for this. ","metadata":{}},{"cell_type":"code","source":"drop_col = ['Super_Bowl','Labor_Day','Thanksgiving','Christmas']\ndf_new.drop(drop_col, axis=1, inplace=True) # dropping columns","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:37:29.199777Z","iopub.execute_input":"2022-04-22T05:37:29.200071Z","iopub.status.idle":"2022-04-22T05:37:29.225389Z","shell.execute_reply.started":"2022-04-22T05:37:29.200032Z","shell.execute_reply":"2022-04-22T05:37:29.224545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (12,10))\nsns.heatmap(df_new.corr().abs())    # To see the correlations\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:37:29.229016Z","iopub.execute_input":"2022-04-22T05:37:29.229266Z","iopub.status.idle":"2022-04-22T05:37:30.099387Z","shell.execute_reply.started":"2022-04-22T05:37:29.229237Z","shell.execute_reply":"2022-04-22T05:37:30.098625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Temperature, unemployment, CPI have no significant effect on weekly sales, so I will drop them. Also, Markdown 4 and 5 highly correlated with Markdown 1. So, I will drop them also. It can create multicollinearity problem, maybe. So, first I will try without them.","metadata":{}},{"cell_type":"code","source":"drop_col = ['Temperature','MarkDown4','MarkDown5','CPI','Unemployment']\ndf_new.drop(drop_col, axis=1, inplace=True) # dropping columns","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:37:30.100972Z","iopub.execute_input":"2022-04-22T05:37:30.101485Z","iopub.status.idle":"2022-04-22T05:37:30.124465Z","shell.execute_reply.started":"2022-04-22T05:37:30.101441Z","shell.execute_reply":"2022-04-22T05:37:30.123058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (12,10))\nsns.heatmap(df_new.corr().abs())    # To see the correlations without dropping columns\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:37:30.126136Z","iopub.execute_input":"2022-04-22T05:37:30.126492Z","iopub.status.idle":"2022-04-22T05:37:30.905279Z","shell.execute_reply.started":"2022-04-22T05:37:30.126447Z","shell.execute_reply":"2022-04-22T05:37:30.904488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Size and type are highly correlated with weekly sales. Also, department and store are correlated with sales.","metadata":{}},{"cell_type":"code","source":"df_new = df_new.sort_values(by='Date', ascending=True) # sorting according to date","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:37:30.909615Z","iopub.execute_input":"2022-04-22T05:37:30.911711Z","iopub.status.idle":"2022-04-22T05:37:30.997619Z","shell.execute_reply.started":"2022-04-22T05:37:30.911667Z","shell.execute_reply":"2022-04-22T05:37:30.996653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating Train-Test Splits","metadata":{}},{"cell_type":"markdown","source":"Our date column has continuos values, to keep the date features continue, I will not take random splitting. so, I split data manually according to 70%.","metadata":{}},{"cell_type":"code","source":"train_data = df_new[:int(0.7*(len(df_new)))] # taking train part\ntest_data = df_new[int(0.7*(len(df_new))):] # taking test part\n\ntarget = \"Weekly_Sales\"\nused_cols = [c for c in df_new.columns.to_list() if c not in [target]] # all columns except weekly sales\n\nX_train = train_data[used_cols]\nX_test = test_data[used_cols]\ny_train = train_data[target]\ny_test = test_data[target]","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:37:30.998925Z","iopub.execute_input":"2022-04-22T05:37:30.999346Z","iopub.status.idle":"2022-04-22T05:37:31.022195Z","shell.execute_reply.started":"2022-04-22T05:37:30.999305Z","shell.execute_reply":"2022-04-22T05:37:31.021372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df_new[used_cols] # to keep train and test X values together","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:37:31.027668Z","iopub.execute_input":"2022-04-22T05:37:31.028348Z","iopub.status.idle":"2022-04-22T05:37:31.058069Z","shell.execute_reply.started":"2022-04-22T05:37:31.028308Z","shell.execute_reply":"2022-04-22T05:37:31.057177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have enough information in our date such as week of the year. So, I drop date columns.","metadata":{}},{"cell_type":"code","source":"X_train = X_train.drop(['Date'], axis=1) # dropping date from train\nX_test = X_test.drop(['Date'], axis=1) # dropping date from test","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:37:31.062885Z","iopub.execute_input":"2022-04-22T05:37:31.065006Z","iopub.status.idle":"2022-04-22T05:37:31.09555Z","shell.execute_reply.started":"2022-04-22T05:37:31.064961Z","shell.execute_reply":"2022-04-22T05:37:31.094796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Metric Definition Function","metadata":{}},{"cell_type":"markdown","source":"Our metric is not calculated as default from ready models. It is weighed error so, I will use function below to calculate it.","metadata":{}},{"cell_type":"code","source":"def wmae_test(test, pred): # WMAE for test \n    weights = X_test['IsHoliday'].apply(lambda is_holiday:5 if is_holiday else 1)\n    error = np.sum(weights * np.abs(test - pred), axis=0) / np.sum(weights)\n    return error","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:37:31.100533Z","iopub.execute_input":"2022-04-22T05:37:31.103028Z","iopub.status.idle":"2022-04-22T05:37:31.111279Z","shell.execute_reply.started":"2022-04-22T05:37:31.102975Z","shell.execute_reply":"2022-04-22T05:37:31.110433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest Regressor","metadata":{}},{"cell_type":"markdown","source":"To tune the regressor, I can use gridsearch but it takes too much time for this type of data which has many rows and columns. So, I choose regressor parameters manually. I changed the parameters each time and try to find the best result.","metadata":{}},{"cell_type":"code","source":"rf = RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=-1, max_depth=35,\n                           max_features = 'sqrt',min_samples_split = 10)\n\nfrom sklearn.preprocessing import RobustScaler\nscaler = RobustScaler()\n\n\n\n#making pipe tp use scaler and regressor together\npipe = make_pipeline(scaler,rf)\n\npipe.fit(X_train, y_train)\n\n# predictions on train set\ny_pred = pipe.predict(X_train)\n\n# predictions on test set\ny_pred_test = pipe.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:37:31.113123Z","iopub.execute_input":"2022-04-22T05:37:31.113666Z","iopub.status.idle":"2022-04-22T05:37:44.031199Z","shell.execute_reply.started":"2022-04-22T05:37:31.113622Z","shell.execute_reply":"2022-04-22T05:37:44.030363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wmae_test(y_test, y_pred_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:37:44.032468Z","iopub.execute_input":"2022-04-22T05:37:44.032755Z","iopub.status.idle":"2022-04-22T05:37:44.106989Z","shell.execute_reply.started":"2022-04-22T05:37:44.032701Z","shell.execute_reply":"2022-04-22T05:37:44.1061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the first trial, my weighted error is around 5850.","metadata":{}},{"cell_type":"markdown","source":"# To See Feature Importance","metadata":{}},{"cell_type":"code","source":"X = X.drop(['Date'], axis=1) #dropping date column from X","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:37:44.108567Z","iopub.execute_input":"2022-04-22T05:37:44.108959Z","iopub.status.idle":"2022-04-22T05:37:44.121875Z","shell.execute_reply.started":"2022-04-22T05:37:44.108916Z","shell.execute_reply":"2022-04-22T05:37:44.121155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below code cell was taken from our instructor Bryan Arnold's notebook. I changed the code according to my data and see the plot.","metadata":{}},{"cell_type":"code","source":"importances = rf.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in rf.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Printing the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(X.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plotting the feature importances of the forest\nplt.figure()\nplt.title(\"Feature importances\")\nplt.bar(range(X.shape[1]), importances[indices],\n       color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(X.shape[1]), indices)\nplt.xlim([-1, X.shape[1]])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:37:44.123068Z","iopub.execute_input":"2022-04-22T05:37:44.123703Z","iopub.status.idle":"2022-04-22T05:37:44.528946Z","shell.execute_reply.started":"2022-04-22T05:37:44.123666Z","shell.execute_reply":"2022-04-22T05:37:44.528154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After looking feature importance, I dropped least important 3-4 features and tried the model. I found the best result when I dropped month column which is highly correlated with week.","metadata":{}},{"cell_type":"code","source":"X1_train = X_train.drop(['month'], axis=1) # dropping month\nX1_test = X_test.drop(['month'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:37:44.530488Z","iopub.execute_input":"2022-04-22T05:37:44.530787Z","iopub.status.idle":"2022-04-22T05:37:44.544646Z","shell.execute_reply.started":"2022-04-22T05:37:44.530746Z","shell.execute_reply":"2022-04-22T05:37:44.543849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Again without Month","metadata":{}},{"cell_type":"code","source":"rf = RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=-1, max_depth=35,\n                           max_features = 'sqrt',min_samples_split = 10)\n\nscaler=RobustScaler()\npipe = make_pipeline(scaler,rf)\n\npipe.fit(X1_train, y_train)\n\n# predictions on train set\ny_pred = pipe.predict(X1_train)\n\n# predictions on test set\ny_pred_test = pipe.predict(X1_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:37:44.545714Z","iopub.execute_input":"2022-04-22T05:37:44.546361Z","iopub.status.idle":"2022-04-22T05:37:56.935285Z","shell.execute_reply.started":"2022-04-22T05:37:44.546318Z","shell.execute_reply":"2022-04-22T05:37:56.934507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wmae_test(y_test, y_pred_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:37:56.93857Z","iopub.execute_input":"2022-04-22T05:37:56.938818Z","iopub.status.idle":"2022-04-22T05:37:57.060778Z","shell.execute_reply.started":"2022-04-22T05:37:56.938781Z","shell.execute_reply":"2022-04-22T05:37:57.059658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It gives better results than baseline.","metadata":{}},{"cell_type":"markdown","source":"# Model with Whole Data","metadata":{}},{"cell_type":"markdown","source":"Now, I want to make sure that my model will learn from the columns which I dropped or not. So, I will apply my model to whole encoded data again.","metadata":{}},{"cell_type":"code","source":"# splitting train-test to whole dataset\ntrain_data_enc = df_encoded[:int(0.7*(len(df_encoded)))]\ntest_data_enc = df_encoded[int(0.7*(len(df_encoded))):]\n\ntarget = \"Weekly_Sales\"\nused_cols1 = [c for c in df_encoded.columns.to_list() if c not in [target]] # all columns except price\n\nX_train_enc = train_data_enc[used_cols1]\nX_test_enc = test_data_enc[used_cols1]\ny_train_enc = train_data_enc[target]\ny_test_enc = test_data_enc[target]","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:37:57.065375Z","iopub.execute_input":"2022-04-22T05:37:57.065773Z","iopub.status.idle":"2022-04-22T05:37:57.141154Z","shell.execute_reply.started":"2022-04-22T05:37:57.065716Z","shell.execute_reply":"2022-04-22T05:37:57.140234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_enc = df_encoded[used_cols1] # to get together train,test splits","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:37:57.142371Z","iopub.execute_input":"2022-04-22T05:37:57.142641Z","iopub.status.idle":"2022-04-22T05:37:57.219481Z","shell.execute_reply.started":"2022-04-22T05:37:57.142604Z","shell.execute_reply":"2022-04-22T05:37:57.2183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_enc = X_enc.drop(['Date'], axis=1) #dropping date column for whole X","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:37:57.22107Z","iopub.execute_input":"2022-04-22T05:37:57.221338Z","iopub.status.idle":"2022-04-22T05:37:57.257546Z","shell.execute_reply.started":"2022-04-22T05:37:57.221299Z","shell.execute_reply":"2022-04-22T05:37:57.256693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_enc = X_train_enc.drop(['Date'], axis=1) # dropping date from train and test\nX_test_enc= X_test_enc.drop(['Date'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:37:57.259036Z","iopub.execute_input":"2022-04-22T05:37:57.259445Z","iopub.status.idle":"2022-04-22T05:37:57.28366Z","shell.execute_reply.started":"2022-04-22T05:37:57.259404Z","shell.execute_reply":"2022-04-22T05:37:57.282944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf = RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=-1, max_depth=35,\n                           max_features = 'sqrt',min_samples_split = 10)\n\nscaler=RobustScaler()\npipe = make_pipeline(scaler,rf)\n\npipe.fit(X_train_enc, y_train_enc)\n\n# predictions on train set\ny_pred_enc = pipe.predict(X_train_enc)\n\n# predictions on test set\ny_pred_test_enc = pipe.predict(X_test_enc)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:37:57.285077Z","iopub.execute_input":"2022-04-22T05:37:57.285484Z","iopub.status.idle":"2022-04-22T05:38:11.97864Z","shell.execute_reply.started":"2022-04-22T05:37:57.285445Z","shell.execute_reply":"2022-04-22T05:38:11.977805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wmae_test(y_test_enc, y_pred_test_enc)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:11.980208Z","iopub.execute_input":"2022-04-22T05:38:11.980483Z","iopub.status.idle":"2022-04-22T05:38:12.082441Z","shell.execute_reply.started":"2022-04-22T05:38:11.980445Z","shell.execute_reply":"2022-04-22T05:38:12.081507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We found better results for whole data, it means our model can learn from columns which I dropped before.","metadata":{}},{"cell_type":"markdown","source":"# Feature Importance for Whole Encoded Dataset","metadata":{}},{"cell_type":"code","source":"importances = rf.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in rf.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Printing the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(X_enc.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plotting the feature importances of the forest\nplt.figure()\nplt.title(\"Feature importances\")\nplt.bar(range(X_enc.shape[1]), importances[indices],\n       color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(X_enc.shape[1]), indices)\nplt.xlim([-1, X_enc.shape[1]])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:12.083964Z","iopub.execute_input":"2022-04-22T05:38:12.084412Z","iopub.status.idle":"2022-04-22T05:38:12.548568Z","shell.execute_reply.started":"2022-04-22T05:38:12.08437Z","shell.execute_reply":"2022-04-22T05:38:12.547791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"According to feature importance, I dropped some columns from whole set and try my model again.","metadata":{}},{"cell_type":"code","source":"df_encoded_new = df_encoded.copy() # taking copy of encoded data to keep it without change.\ndf_encoded_new.drop(drop_col, axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:12.549825Z","iopub.execute_input":"2022-04-22T05:38:12.550115Z","iopub.status.idle":"2022-04-22T05:38:12.599322Z","shell.execute_reply.started":"2022-04-22T05:38:12.550074Z","shell.execute_reply":"2022-04-22T05:38:12.598429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model According to Feature Importance","metadata":{}},{"cell_type":"code","source":"#train-test splitting\ntrain_data_enc_new = df_encoded_new[:int(0.7*(len(df_encoded_new)))]\ntest_data_enc_new = df_encoded_new[int(0.7*(len(df_encoded_new))):]\n\ntarget = \"Weekly_Sales\"\nused_cols2 = [c for c in df_encoded_new.columns.to_list() if c not in [target]] # all columns except price\n\nX_train_enc1 = train_data_enc_new[used_cols2]\nX_test_enc1 = test_data_enc_new[used_cols2]\ny_train_enc1 = train_data_enc_new[target]\ny_test_enc1 = test_data_enc_new[target]\n\n#droping date from train-test\nX_train_enc1 = X_train_enc1.drop(['Date'], axis=1)\nX_test_enc1= X_test_enc1.drop(['Date'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:12.600571Z","iopub.execute_input":"2022-04-22T05:38:12.601316Z","iopub.status.idle":"2022-04-22T05:38:12.643547Z","shell.execute_reply.started":"2022-04-22T05:38:12.601262Z","shell.execute_reply":"2022-04-22T05:38:12.64267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf = RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=-1, max_depth=40,\n                           max_features = 'log2',min_samples_split = 10)\n\nscaler=RobustScaler()\npipe = make_pipeline(scaler,rf)\n\npipe.fit(X_train_enc1, y_train_enc1)\n\n# predictions on train set\ny_pred_enc = pipe.predict(X_train_enc1)\n\n# predictions on test set\ny_pred_test_enc = pipe.predict(X_test_enc1)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:12.644884Z","iopub.execute_input":"2022-04-22T05:38:12.645597Z","iopub.status.idle":"2022-04-22T05:38:25.583335Z","shell.execute_reply.started":"2022-04-22T05:38:12.645551Z","shell.execute_reply":"2022-04-22T05:38:25.582507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe.score(X_test_enc1,y_test_enc1)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:25.584569Z","iopub.execute_input":"2022-04-22T05:38:25.584878Z","iopub.status.idle":"2022-04-22T05:38:26.0136Z","shell.execute_reply.started":"2022-04-22T05:38:25.584839Z","shell.execute_reply":"2022-04-22T05:38:26.012882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wmae_test(y_test_enc1, y_pred_test_enc)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:26.015421Z","iopub.execute_input":"2022-04-22T05:38:26.015723Z","iopub.status.idle":"2022-04-22T05:38:26.1176Z","shell.execute_reply.started":"2022-04-22T05:38:26.015682Z","shell.execute_reply":"2022-04-22T05:38:26.116606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I found best results with doing feature selection from whole encoded dataset.","metadata":{}},{"cell_type":"markdown","source":"# Model with Dropping Month Column","metadata":{}},{"cell_type":"markdown","source":"With the same dateset before, I try to model again without month column. ","metadata":{}},{"cell_type":"code","source":"df_encoded_new1 = df_encoded.copy()\ndf_encoded_new1.drop(drop_col, axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:26.119422Z","iopub.execute_input":"2022-04-22T05:38:26.119717Z","iopub.status.idle":"2022-04-22T05:38:26.168473Z","shell.execute_reply.started":"2022-04-22T05:38:26.119678Z","shell.execute_reply":"2022-04-22T05:38:26.167639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_encoded_new1 = df_encoded_new1.drop(['Date'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:26.170115Z","iopub.execute_input":"2022-04-22T05:38:26.170419Z","iopub.status.idle":"2022-04-22T05:38:26.19345Z","shell.execute_reply.started":"2022-04-22T05:38:26.170377Z","shell.execute_reply":"2022-04-22T05:38:26.192601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_encoded_new1 = df_encoded_new1.drop(['month'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:26.194805Z","iopub.execute_input":"2022-04-22T05:38:26.195386Z","iopub.status.idle":"2022-04-22T05:38:26.225548Z","shell.execute_reply.started":"2022-04-22T05:38:26.195341Z","shell.execute_reply":"2022-04-22T05:38:26.224625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train-test split\ntrain_data_enc_new1 = df_encoded_new1[:int(0.7*(len(df_encoded_new1)))]\ntest_data_enc_new1 = df_encoded_new1[int(0.7*(len(df_encoded_new1))):]\n\ntarget = \"Weekly_Sales\"\nused_cols3 = [c for c in df_encoded_new1.columns.to_list() if c not in [target]] # all columns except price\n\nX_train_enc2 = train_data_enc_new1[used_cols3]\nX_test_enc2 = test_data_enc_new1[used_cols3]\ny_train_enc2 = train_data_enc_new1[target]\ny_test_enc2 = test_data_enc_new1[target]","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:26.227236Z","iopub.execute_input":"2022-04-22T05:38:26.227677Z","iopub.status.idle":"2022-04-22T05:38:26.252212Z","shell.execute_reply.started":"2022-04-22T05:38:26.227589Z","shell.execute_reply":"2022-04-22T05:38:26.251397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#modeling part\npipe = make_pipeline(scaler,rf)\n\npipe.fit(X_train_enc2, y_train_enc2)\n\n# predictions on train set\ny_pred_enc = pipe.predict(X_train_enc2)\n\n# predictions on test set\ny_pred_test_enc = pipe.predict(X_test_enc2)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:26.253589Z","iopub.execute_input":"2022-04-22T05:38:26.253965Z","iopub.status.idle":"2022-04-22T05:38:37.811417Z","shell.execute_reply.started":"2022-04-22T05:38:26.253922Z","shell.execute_reply":"2022-04-22T05:38:37.810569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe.score(X_test_enc2,y_test_enc2)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:37.812995Z","iopub.execute_input":"2022-04-22T05:38:37.813304Z","iopub.status.idle":"2022-04-22T05:38:38.241434Z","shell.execute_reply.started":"2022-04-22T05:38:37.813262Z","shell.execute_reply":"2022-04-22T05:38:38.240697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wmae_test(y_test_enc2, y_pred_test_enc)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:38.242837Z","iopub.execute_input":"2022-04-22T05:38:38.243103Z","iopub.status.idle":"2022-04-22T05:38:38.344527Z","shell.execute_reply.started":"2022-04-22T05:38:38.243064Z","shell.execute_reply":"2022-04-22T05:38:38.34378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It did not give better results than before.","metadata":{}},{"cell_type":"code","source":"df_results = pd.DataFrame(columns=[\"Model\", \"Info\",'WMAE']) # result df for showing results together","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:38.345915Z","iopub.execute_input":"2022-04-22T05:38:38.346188Z","iopub.status.idle":"2022-04-22T05:38:38.353015Z","shell.execute_reply.started":"2022-04-22T05:38:38.346148Z","shell.execute_reply":"2022-04-22T05:38:38.35223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# writing results to df\ndf_results = df_results.append({     \n     \"Model\": 'RandomForestRegressor' ,\n      \"Info\": 'w/out divided holiday columns' , \n       'WMAE' : 5850}, ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:38.354645Z","iopub.execute_input":"2022-04-22T05:38:38.355194Z","iopub.status.idle":"2022-04-22T05:38:38.365997Z","shell.execute_reply.started":"2022-04-22T05:38:38.355154Z","shell.execute_reply":"2022-04-22T05:38:38.365127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_results = df_results.append({     \n     \"Model\": 'RandomForestRegressor' ,\n      \"Info\": 'w/out month column' , \n       'WMAE' : 5494}, ignore_index=True)\ndf_results = df_results.append({     \n     \"Model\": 'RandomForestRegressor' ,\n      \"Info\": 'whole data' , \n       'WMAE' : 2450}, ignore_index=True)\ndf_results = df_results.append({     \n     \"Model\": 'RandomForestRegressor' ,\n      \"Info\": 'whole data with feature selection' , \n       'WMAE' : 1801}, ignore_index=True)\ndf_results = df_results.append({     \n     \"Model\": 'RandomForestRegressor' ,\n      \"Info\": 'whole data with feature selection w/out month' , \n       'WMAE' : 2093}, ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:38.367556Z","iopub.execute_input":"2022-04-22T05:38:38.368106Z","iopub.status.idle":"2022-04-22T05:38:38.386722Z","shell.execute_reply.started":"2022-04-22T05:38:38.368065Z","shell.execute_reply":"2022-04-22T05:38:38.385883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_results","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:38.388389Z","iopub.execute_input":"2022-04-22T05:38:38.389012Z","iopub.status.idle":"2022-04-22T05:38:38.401623Z","shell.execute_reply.started":"2022-04-22T05:38:38.388969Z","shell.execute_reply":"2022-04-22T05:38:38.400687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The best results belongs to whole data set with feature selection. Now, I will try time series models.","metadata":{}},{"cell_type":"markdown","source":"# Time Series Models","metadata":{}},{"cell_type":"code","source":"df.head() # to see my data","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:38.403959Z","iopub.execute_input":"2022-04-22T05:38:38.404258Z","iopub.status.idle":"2022-04-22T05:38:38.432817Z","shell.execute_reply.started":"2022-04-22T05:38:38.404217Z","shell.execute_reply":"2022-04-22T05:38:38.431703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"Date\"] = pd.to_datetime(df[\"Date\"]) #changing data to datetime for decomposing","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:38.434412Z","iopub.execute_input":"2022-04-22T05:38:38.43475Z","iopub.status.idle":"2022-04-22T05:38:38.464842Z","shell.execute_reply.started":"2022-04-22T05:38:38.434693Z","shell.execute_reply":"2022-04-22T05:38:38.464082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.set_index('Date', inplace=True) #seting date as index","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:38.466875Z","iopub.execute_input":"2022-04-22T05:38:38.467279Z","iopub.status.idle":"2022-04-22T05:38:38.473106Z","shell.execute_reply.started":"2022-04-22T05:38:38.46724Z","shell.execute_reply":"2022-04-22T05:38:38.472328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plotting Sales","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16,6))\ndf['Weekly_Sales'].plot()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:38.476337Z","iopub.execute_input":"2022-04-22T05:38:38.476807Z","iopub.status.idle":"2022-04-22T05:38:42.15739Z","shell.execute_reply.started":"2022-04-22T05:38:38.476757Z","shell.execute_reply":"2022-04-22T05:38:42.156602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this data, there are lots of same data values. So, I will collect them together as weekly.","metadata":{}},{"cell_type":"code","source":"df_week = df.resample('W').mean() #resample data as weekly","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:42.158955Z","iopub.execute_input":"2022-04-22T05:38:42.159476Z","iopub.status.idle":"2022-04-22T05:38:42.357707Z","shell.execute_reply.started":"2022-04-22T05:38:42.15943Z","shell.execute_reply":"2022-04-22T05:38:42.356906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16,6))\ndf_week['Weekly_Sales'].plot()\nplt.title('Average Sales - Weekly')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:42.359261Z","iopub.execute_input":"2022-04-22T05:38:42.359532Z","iopub.status.idle":"2022-04-22T05:38:42.682368Z","shell.execute_reply.started":"2022-04-22T05:38:42.359494Z","shell.execute_reply":"2022-04-22T05:38:42.681597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With the collecting data as weekly, I can see average sales clearly. To see monthly pattern , I resampled my data to monthly also.","metadata":{}},{"cell_type":"code","source":"df_month = df.resample('MS').mean() # resampling as monthly","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:42.683893Z","iopub.execute_input":"2022-04-22T05:38:42.684372Z","iopub.status.idle":"2022-04-22T05:38:42.827764Z","shell.execute_reply.started":"2022-04-22T05:38:42.684331Z","shell.execute_reply":"2022-04-22T05:38:42.826965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16,6))\ndf_month['Weekly_Sales'].plot()\nplt.title('Average Sales - Monthly')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:42.82932Z","iopub.execute_input":"2022-04-22T05:38:42.829615Z","iopub.status.idle":"2022-04-22T05:38:43.139534Z","shell.execute_reply.started":"2022-04-22T05:38:42.829576Z","shell.execute_reply":"2022-04-22T05:38:43.138808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When I turned data to monthly, I realized that I lost some patterns in weekly data. So, I will continue with weekly resampled data.","metadata":{}},{"cell_type":"markdown","source":"# To Observe 2-weeks Rolling Mean and Std","metadata":{}},{"cell_type":"markdown","source":"My data is non-stationary. So, I will try to find more stationary version on it. ","metadata":{}},{"cell_type":"code","source":"# finding 2-weeks rolling mean and std\nroll_mean = df_week['Weekly_Sales'].rolling(window=2, center=False).mean()\nroll_std = df_week['Weekly_Sales'].rolling(window=2, center=False).std()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:43.150103Z","iopub.execute_input":"2022-04-22T05:38:43.150518Z","iopub.status.idle":"2022-04-22T05:38:43.15837Z","shell.execute_reply.started":"2022-04-22T05:38:43.150467Z","shell.execute_reply":"2022-04-22T05:38:43.157558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(13, 6))\nax.plot(df_week['Weekly_Sales'], color='blue',label='Average Weekly Sales')\nax.plot(roll_mean, color='red', label='Rolling 2-Week Mean')\nax.plot(roll_std, color='black', label='Rolling 2-Week Standard Deviation')\nax.legend()\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:43.160999Z","iopub.execute_input":"2022-04-22T05:38:43.161645Z","iopub.status.idle":"2022-04-22T05:38:43.56638Z","shell.execute_reply.started":"2022-04-22T05:38:43.161599Z","shell.execute_reply":"2022-04-22T05:38:43.565614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Adfuller Test to Make Sure","metadata":{}},{"cell_type":"code","source":"adfuller(df_week['Weekly_Sales'])","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:43.567716Z","iopub.execute_input":"2022-04-22T05:38:43.568574Z","iopub.status.idle":"2022-04-22T05:38:43.589456Z","shell.execute_reply.started":"2022-04-22T05:38:43.568528Z","shell.execute_reply":"2022-04-22T05:38:43.588664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From test and my observations my data is not stationary. So, I will try to find more stationary version of it.","metadata":{}},{"cell_type":"markdown","source":"# Train - Test Split of Weekly Data","metadata":{}},{"cell_type":"markdown","source":"To take train-test splits continuosly, I split them manually, not random.","metadata":{}},{"cell_type":"code","source":"train_data = df_week[:int(0.7*(len(df_week)))] \ntest_data = df_week[int(0.7*(len(df_week))):]\n\nprint('Train:', train_data.shape)\nprint('Test:', test_data.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:43.590883Z","iopub.execute_input":"2022-04-22T05:38:43.591175Z","iopub.status.idle":"2022-04-22T05:38:43.599757Z","shell.execute_reply.started":"2022-04-22T05:38:43.591137Z","shell.execute_reply":"2022-04-22T05:38:43.598878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = \"Weekly_Sales\"\nused_cols = [c for c in df_week.columns.to_list() if c not in [target]] # all columns except price\n\n# assigning train-test X-y values\n\nX_train = train_data[used_cols]\nX_test = test_data[used_cols]\ny_train = train_data[target]\ny_test = test_data[target]","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:43.603239Z","iopub.execute_input":"2022-04-22T05:38:43.603712Z","iopub.status.idle":"2022-04-22T05:38:43.61268Z","shell.execute_reply.started":"2022-04-22T05:38:43.603666Z","shell.execute_reply":"2022-04-22T05:38:43.611789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['Weekly_Sales'].plot(figsize=(20,8), title= 'Weekly_Sales', fontsize=14)\ntest_data['Weekly_Sales'].plot(figsize=(20,8), title= 'Weekly_Sales', fontsize=14)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:43.614293Z","iopub.execute_input":"2022-04-22T05:38:43.614618Z","iopub.status.idle":"2022-04-22T05:38:43.975503Z","shell.execute_reply.started":"2022-04-22T05:38:43.61458Z","shell.execute_reply":"2022-04-22T05:38:43.974759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Blue line represents my train data, yellow is test data.","metadata":{}},{"cell_type":"markdown","source":"# Decomposing Weekly Data to Observe Seasonality","metadata":{}},{"cell_type":"code","source":"decomposed = decompose(df_week['Weekly_Sales'].values, 'additive', m=20) #decomposing of weekly data ","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:43.977079Z","iopub.execute_input":"2022-04-22T05:38:43.977378Z","iopub.status.idle":"2022-04-22T05:38:43.983741Z","shell.execute_reply.started":"2022-04-22T05:38:43.977337Z","shell.execute_reply":"2022-04-22T05:38:43.982803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"decomposed_plot(decomposed, figure_kwargs={'figsize': (16, 10)})\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:43.985294Z","iopub.execute_input":"2022-04-22T05:38:43.985875Z","iopub.status.idle":"2022-04-22T05:38:44.485369Z","shell.execute_reply.started":"2022-04-22T05:38:43.985686Z","shell.execute_reply":"2022-04-22T05:38:44.48463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the graphs above, every 20 step seasonality converges to beginning point. This helps me to tune my model.","metadata":{}},{"cell_type":"markdown","source":"# Trying To Make Data More Stationary","metadata":{}},{"cell_type":"markdown","source":"Now, I will try to make my data more stationary. To do this, I will try model with differenced, logged and shifted data.","metadata":{}},{"cell_type":"markdown","source":"## 1. Difference","metadata":{}},{"cell_type":"code","source":"df_week_diff = df_week['Weekly_Sales'].diff().dropna() #creating difference values","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:44.486554Z","iopub.execute_input":"2022-04-22T05:38:44.487466Z","iopub.status.idle":"2022-04-22T05:38:44.494642Z","shell.execute_reply.started":"2022-04-22T05:38:44.487412Z","shell.execute_reply":"2022-04-22T05:38:44.493792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# taking mean and std of differenced data\ndiff_roll_mean = df_week_diff.rolling(window=2, center=False).mean()\ndiff_roll_std = df_week_diff.rolling(window=2, center=False).std()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:44.496406Z","iopub.execute_input":"2022-04-22T05:38:44.49671Z","iopub.status.idle":"2022-04-22T05:38:44.506499Z","shell.execute_reply.started":"2022-04-22T05:38:44.496672Z","shell.execute_reply":"2022-04-22T05:38:44.505675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(13, 6))\nax.plot(df_week_diff, color='blue',label='Difference')\nax.plot(diff_roll_mean, color='red', label='Rolling Mean')\nax.plot(diff_roll_std, color='black', label='Rolling Standard Deviation')\nax.legend()\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:44.50814Z","iopub.execute_input":"2022-04-22T05:38:44.508447Z","iopub.status.idle":"2022-04-22T05:38:44.950399Z","shell.execute_reply.started":"2022-04-22T05:38:44.508397Z","shell.execute_reply":"2022-04-22T05:38:44.949579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.Shift","metadata":{}},{"cell_type":"code","source":"df_week_lag = df_week['Weekly_Sales'].shift().dropna() #shifting the data ","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:44.951967Z","iopub.execute_input":"2022-04-22T05:38:44.952716Z","iopub.status.idle":"2022-04-22T05:38:44.958685Z","shell.execute_reply.started":"2022-04-22T05:38:44.952671Z","shell.execute_reply":"2022-04-22T05:38:44.957802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lag_roll_mean = df_week_lag.rolling(window=2, center=False).mean() \nlag_roll_std = df_week_lag.rolling(window=2, center=False).std()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:44.960362Z","iopub.execute_input":"2022-04-22T05:38:44.96065Z","iopub.status.idle":"2022-04-22T05:38:44.970318Z","shell.execute_reply.started":"2022-04-22T05:38:44.960603Z","shell.execute_reply":"2022-04-22T05:38:44.969462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(13, 6))\nax.plot(df_week_lag, color='blue',label='Difference')\nax.plot(lag_roll_mean, color='red', label='Rolling Mean')\nax.plot(lag_roll_std, color='black', label='Rolling Standard Deviation')\nax.legend()\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:44.971852Z","iopub.execute_input":"2022-04-22T05:38:44.972159Z","iopub.status.idle":"2022-04-22T05:38:45.371029Z","shell.execute_reply.started":"2022-04-22T05:38:44.972111Z","shell.execute_reply":"2022-04-22T05:38:45.370287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.Log","metadata":{}},{"cell_type":"code","source":"logged_week = np.log1p(df_week['Weekly_Sales']).dropna() #taking log of data","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:45.372184Z","iopub.execute_input":"2022-04-22T05:38:45.372943Z","iopub.status.idle":"2022-04-22T05:38:45.378972Z","shell.execute_reply.started":"2022-04-22T05:38:45.3729Z","shell.execute_reply":"2022-04-22T05:38:45.377934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_roll_mean = logged_week.rolling(window=2, center=False).mean()\nlog_roll_std = logged_week.rolling(window=2, center=False).std()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:45.38032Z","iopub.execute_input":"2022-04-22T05:38:45.38095Z","iopub.status.idle":"2022-04-22T05:38:45.390052Z","shell.execute_reply.started":"2022-04-22T05:38:45.380902Z","shell.execute_reply":"2022-04-22T05:38:45.389309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(13, 6))\nax.plot(logged_week, color='blue',label='Logged')\nax.plot(log_roll_mean, color='red', label='Rolling Mean')\nax.plot(log_roll_std, color='black', label='Rolling Standard Deviation')\nax.legend()\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:45.392632Z","iopub.execute_input":"2022-04-22T05:38:45.393774Z","iopub.status.idle":"2022-04-22T05:38:45.775087Z","shell.execute_reply.started":"2022-04-22T05:38:45.393719Z","shell.execute_reply":"2022-04-22T05:38:45.774296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Auto-ARIMA MODEL","metadata":{}},{"cell_type":"markdown","source":"I tried my data without any changes, then tried with shifting, taking log and difference version of data. Differenced data gave best results. So, I decided to take difference and use this data. ","metadata":{}},{"cell_type":"markdown","source":"# Train-Test Split","metadata":{}},{"cell_type":"code","source":"train_data_diff = df_week_diff [:int(0.7*(len(df_week_diff )))]\ntest_data_diff = df_week_diff [int(0.7*(len(df_week_diff ))):]","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:45.77626Z","iopub.execute_input":"2022-04-22T05:38:45.777036Z","iopub.status.idle":"2022-04-22T05:38:45.78327Z","shell.execute_reply.started":"2022-04-22T05:38:45.776984Z","shell.execute_reply":"2022-04-22T05:38:45.782288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_data = train_data['Weekly_Sales']\n# test_data = test_data['Weekly_Sales']\n\nmodel_auto_arima = auto_arima(train_data_diff, trace=True,start_p=0, start_q=0, start_P=0, start_Q=0,\n                  max_p=20, max_q=20, max_P=20, max_Q=20, seasonal=True,maxiter=200,\n                  information_criterion='aic',stepwise=False, suppress_warnings=True, D=1, max_D=10,\n                  error_action='ignore',approximation = False)\nmodel_auto_arima.fit(train_data_diff)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:45.785074Z","iopub.execute_input":"2022-04-22T05:38:45.785391Z","iopub.status.idle":"2022-04-22T05:38:51.89804Z","shell.execute_reply.started":"2022-04-22T05:38:45.785347Z","shell.execute_reply":"2022-04-22T05:38:51.897182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model_auto_arima.predict(n_periods=len(test_data_diff))\ny_pred = pd.DataFrame(y_pred,index = test_data.index,columns=['Prediction'])\nplt.figure(figsize=(20,6))\nplt.title('Prediction of Weekly Sales Using Auto-ARIMA', fontsize=20)\nplt.plot(train_data_diff, label='Train')\nplt.plot(test_data_diff, label='Test')\nplt.plot(y_pred, label='Prediction of ARIMA')\nplt.legend(loc='best')\nplt.xlabel('Date', fontsize=14)\nplt.ylabel('Weekly Sales', fontsize=14)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:51.899397Z","iopub.execute_input":"2022-04-22T05:38:51.899921Z","iopub.status.idle":"2022-04-22T05:38:52.434209Z","shell.execute_reply.started":"2022-04-22T05:38:51.899877Z","shell.execute_reply":"2022-04-22T05:38:52.433486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I do not like the pattern of predictions so I decided to try another model.","metadata":{}},{"cell_type":"markdown","source":"# ExponentialSmoothing","metadata":{}},{"cell_type":"markdown","source":"I checked suitable Holt-Winters models according tp my data. Exponential Smooting are used when data has trend, and it flattens the trend. The damped trend method adds a damping parameter so, the trend converges to a constant value in the future. ","metadata":{}},{"cell_type":"markdown","source":"My difference data has some minus and zero values, so I used additive seasonal and trend instead of multiplicative. Seasonal periods are chosen from the decomposed graphs above. For tuning the model with iterations take too much time so, I changed and tried model for different parameters and found the best parameters and fitted them to model.","metadata":{}},{"cell_type":"code","source":"model_holt_winters = ExponentialSmoothing(train_data_diff, seasonal_periods=20, seasonal='additive',\n                                           trend='additive',damped=True).fit() #Taking additive trend and seasonality.\ny_pred = model_holt_winters.forecast(len(test_data_diff))# Predict the test data\n\n#Visualize train, test and predicted data.\nplt.figure(figsize=(20,6))\nplt.title('Prediction of Weekly Sales using ExponentialSmoothing', fontsize=20)\nplt.plot(train_data_diff, label='Train')\nplt.plot(test_data_diff, label='Test')\nplt.plot(y_pred, label='Prediction using ExponentialSmoothing')\nplt.legend(loc='best')\nplt.xlabel('Date', fontsize=14)\nplt.ylabel('Weekly Sales', fontsize=14)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:52.438389Z","iopub.execute_input":"2022-04-22T05:38:52.440476Z","iopub.status.idle":"2022-04-22T05:38:52.98293Z","shell.execute_reply.started":"2022-04-22T05:38:52.440433Z","shell.execute_reply":"2022-04-22T05:38:52.98218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wmae_test(test_data_diff, y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T05:38:52.98415Z","iopub.execute_input":"2022-04-22T05:38:52.985031Z","iopub.status.idle":"2022-04-22T05:38:52.993858Z","shell.execute_reply.started":"2022-04-22T05:38:52.984985Z","shell.execute_reply":"2022-04-22T05:38:52.993043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At the end, I found best results for my data with Exponential Smoothing Model.","metadata":{}},{"cell_type":"markdown","source":"My best result for this project is 821. According to sales amounts this value is roughly around 4-5% error. If we can take our average sales and take percentage of 821 errors, it gives 4-5% roughly. ","metadata":{}}]}